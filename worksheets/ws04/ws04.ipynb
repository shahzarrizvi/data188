{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "from prob140 import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 4 #\n",
    "All the libraries that you might reasonably want to use for the calculations in Exercises 2 and 3 are imported in the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Towards the Asymptotic Normality of the Sample Correlation\n",
    "Let $(X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)$ be i.i.d. pairs.\n",
    "\n",
    "**(a)** Let $a, b, c, d$ be constants and assume $b$ and $d$ are non-zero. Identify the shape of the asymptotic distribution of \n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{X_i - a}{b} \\right)\\left( \\frac{Y_i - c}{d} \\right)\n",
    "$$\n",
    "\n",
    "and justify your choice. You don't have to find the asymptotic mean and variance.\n",
    "\n",
    "**(b)** Refer to Worksheet 3 for notation for the sample mean and sample variance and define the [sample correlation coefficient](https://inferentialthinking.com/chapters/15/1/Correlation.html#calculating-r)\n",
    "\n",
    "$$\n",
    "R_n ~ = ~ \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{X_i - \\bar{X}_n}{\\hat{\\sigma}_X} \\right)\\left( \\frac{Y_i - \\bar{Y}_n}{\\hat{\\sigma}_Y} \\right).\n",
    "$$\n",
    "\n",
    "Discuss why the asymptotic distribution of $R_n$ should have the same shape as the one you identified in Part **(a)**, apart from the mean and variance. You don't have the tools to provide a complete argument. Discuss some issues that a complete argument would have to account for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nonparametric Bootstrap and Confidence Intervals\n",
    "In the nonparametric bootstrap, which is what we use in Data 8 and 100, the data $X_1, X_2, \\ldots, X_n$ are assumed to be i.i.d. from some underlying distribution but nothing further is assumed about that distribution. In particular, we don't assume that is comes from any parametric family such as normal or exponential.\n",
    "\n",
    "In this exercise you will construct boostrap confidence intervals for an underlying correlation, using the percentile method as well as the basic bootstrap method described in seminar.\n",
    "\n",
    "The Data 8 table `births` contains data on a random sample of mother-newborn pairs. You can assume that the sample is like a random sample drawn with replacement from a large underlying population. Let $\\rho$ be the correlation between the newborns' birthweights and their mothers' heights in the underlying population. You will use the boostrap to estimate $\\rho$.\n",
    "\n",
    "**Useful code:** For two arrays `x` and `y`, the expression `np.corrcoef(x, y)` evaluates to a $2 \\times 2$ correlation matrix that has $1$ on the diagonal and ${\\rm Corr}(x, y)$ as the two off-diagonal elements. So `np.corrcoef(x, y)[0, 1]` can be used to get the correlation which is the $(0, 1)$ element of the matrix.\n",
    "\n",
    "**(a)** Find $\\hat{\\rho}$, the correlation between the birth weights and mothers' heights in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (a)\n",
    "births = pd.read_csv('births.csv')\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Now bootstrap the sample $B = 10,000$ times by resampling from the original sample as in Data 8 and 100. Each time, find the correlation of birth weights and mothers' heights in the new sample, and collect all these correlations in an array or other similar form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (b)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Bootstrap Percentile Method:** As in Data 8, draw the empirical histogram of all the simulated values, and find an approximate 95% confidence interval for $\\rho$ by using the appropriate percentiles of the empirical distribution of your estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (c)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Basic or Empirical or Pivotal Bootstrap Method:** Yes, it does have a lot of names. Subtract $\\hat{\\rho}$ from each of your estimates in Part **(b)** and draw a histogram of these deviations. What value do you notice near the center of the distribution? Provide the appropriate percentiles of these deviations and use them along with $\\hat{\\rho}$ to construct an approximate 95% confidence for $\\rho$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (d)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Provide a brief comparison of the intervals in Parts **(c)** and **(d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parametric Bootstrap and Confidence Intervals\n",
    "The parametric bootstrap method assumes $X_1, X_2, \\ldots, X_n$ are i.i.d. from a parametric family with density $f(x \\mid \\theta)$. So the parametric bootstrap doesn't need to resample from the original sample. Instead, it constructs an estimate $\\hat{\\theta}$ based on the original sample and then creates new samples by drawing repeatedly from $f(x \\mid \\hat{\\theta})$. \n",
    "\n",
    "In this exercise you will construct a parametric bootstrap confidence interval for an exponential rate and compare it with the corresponding interval based on the asymptotic distribution of the MLE.\n",
    "\n",
    "**Useful code:** To simulate `n` draws from the exponential distribution with rate `lam`, use `stats.expon.rvs(size = n, scale = 1/lam)`. \n",
    "\n",
    "The array `expon_sample` contains the results of 400 i.i.d. draws from the exponential distribution with unknown rate $\\lambda$. For $n = 400$, these are the observed values of $X_1, X_2, \\ldots, X_n$ drawn independently from the exponential $(\\lambda)$ distribution.\n",
    "\n",
    "**(a)** Use `expon_sample` to find the observed value of $\\hat{\\lambda}$, the MLE of $\\lambda$. This is the value of some function of your sample. Let's call it $T(X_1, X_2, \\ldots, X_n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Part (a)\n",
    "expon_sample = np.load('expon_sample.npy')\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Now pretend you don't know any more MLE theory, and construct $B = 10,000$ parametric bootstrap estimates of $\\lambda$ as follows.\n",
    "\n",
    "- Do the following $B$ times and collect the results in an array or other similar form:\n",
    "    - Generate 400 i.i.d. draws from the exponential distribution with the rate you found in Part **(a)**. Call this new sample $X_1^*, X_2^*, \\ldots, X_n^*$.\n",
    "    - Use the new sample and construct a new estimate of $\\lambda$ as you did in Part **(a)**. That is, find the value of $T(X_1^*, X_2^*, \\ldots, X_n^*)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (b)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Draw an empirical histogram of your estimates and check that its shape resembles what you'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (c)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Construct an approximate 95% empirical bootstrap confidence interval for $\\lambda$ based on your $B$ estimates. See Part **(d)** of Exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (d)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Return to the original sample and use MLE theory to construct an approximate 95% confidence interval for $\\lambda$. Do not use your bootstrap estimates here. Instead, refer to Exercise 1 of Worksheet 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (e)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** Provide a brief comparison of the intervals in Parts **(d)** and **(e)**. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
